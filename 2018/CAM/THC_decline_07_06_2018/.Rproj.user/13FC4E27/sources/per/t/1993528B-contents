# Exponential decay model
# Written by Kevin Potter
# email: kevin.w.potter@gmail.com
# Please email me directly if you 
# have any questions or comments
# Last updated 2018-10-16

# Table of contents
# 1) Initial setup
# 2) Define functions
#   2.1) ed_model
#   2.2) mle_ed_tobit
#   2.3) estimate_ed_lm
#   2.4) plot_decay
#   2.5) estimate_ed_brms
# 3) Plot of THCCOOH by day
# 4) Data imputation based on tobit regression
# 5) Bayesian population-level model (example)
# 6) Nested cross-validation approach
#   6.1) extract_brm_res
#   6.2) compute_coverage_prob
#   6.3) compute_R2
#   6.4) Fit models to data
# 7) Best-fitting model
# 8) Model results
#   8.1) Functions for summarizing results
#   8.2) Load in results
#   8.3) Half-life
#   8.4) Window of detection
#   8.5) Starting level of CN-THCCOOH
#   8.6) Correlations between dip-stick test and PK parameters
#   8.7) Figure 1
#   8.8) Figure 2
#   8.9) Figure S.1
#   8.10) Figure S.2
# 9) CUDIT and withdrawal variables

###
### 1) Initial setup
###

source( 'S01_Folder_paths.R' )

# Indicate which code segments to run
run_code = c(
  F, # 1. Plot of THCCOOH by day
  T, # 2. Data imputation based on tobit regression
  F, # 3. Bayesian population-level model (example)
  T, # 4. Nested cross-validation approach
  F, # 5. Best-fitting model
  F, # 6. Model results
  F  # 7. CUDIT and withdrawal variables
)

# Indicate whether figures should be saved as a PDF file
save_plot = F

# Load in useful packages

# Collection of useful functions
my_package_load( 'utilityf', github = T )

# Package for working with data frames
my_package_load( 'dplyr' )

# Package for Bayesian estimation
my_package_load( 'brms' )

# Package for estimating bayes factors
my_package_load( 'BayesFactor' )

# Make sure 'loo' package is installed
# install.packages( 'loo' )

# Load in data
setwd( dat_dir )
load( 'THC_decay.RData' )

# Compute summed CUDIT score
all_dat$CUDIT.SS = 
  rowSums( all_dat[,grep('CUDIT',colnames(all_dat))] )
for ( s in 1:length( unique( all_dat$ID ) ) ) {
  sel = all_dat$ID == unique( all_dat$ID )[s]
  val =  all_dat$CUDIT.SS[sel]
  val = val[ !is.na( val ) ]
  if ( length( val ) > 0 )
    all_dat$CUDIT.SS[sel] = val
}

# Convert days from baseline measurement 
# to days from THC ingestion
cd = all_dat[ all_dat$Data_issues == '0', ]
cd$Days_from_baseline = cd$Time
cd$Time = cd$Time + cd$Recency_of_MJ_use

###
### 2) Define functions
###

# 2.1)
ed_model = function( param, x, log = F ) {
  # Purpose:
  # Computes the predicted THCCOOH values 
  # using an exponential decay model.
  # Arguments:
  # param - A vector with the starting level of 
  #         THCCOOH and the elimation rate
  # x     - A vector with the days since THC 
  #         ingestion
  # log   - Logical; if TRUE, returns the 
  #         log THCCOOH values instead
  # Returns:
  # The predicted log THCCOOH levels per each day.
  
  yhat = log( param[1] ) - param[2] * x
  if ( !log ) yhat = exp( yhat )
  
  return( yhat )
}

# 2.2)
mle_ed_tobit = function( param, dat ) {
  # Purpose:
  # Computes the negative of the summed log-likelihoods 
  # for a tobit regression approach to fit an 
  # exponential decay model with log-normal error.
  # Arguments:
  # param - The starting value, elimination rate, and the 
  #         residual standard deviation parameters.
  # dat   - A list with the days since THC ingestion, 
  #         the THCCOOH levels, and the cut-off below 
  #         which values are rounded down to 0.
  # Returns:
  # The negative of the summed log-likelihoods.
  
  # Extract data
  x = dat$x # Independent variable
  y = dat$y # Dependent variable
  a = dat$a # Cut-off for censored data
  
  # Indicator function for censored data
  I_a = y < a
  y_star = y; y_star[ I_a ] = a
  y_star = log( y_star )
  
  # Compute mean
  mu = ed_model( param[1:2], x, log = T )
  # Extract standard devation for residuals
  sigma = param[3]
  
  # Carry out tobit regression
  p1 = I_a * pnorm( ( y_star - mu )/sigma, log.p = T )
  p2 = (1-I_a) * (dnorm( (y_star - mu)/sigma, log = T )- log( sigma ) )
  sll = sum( p1 + p2 )
  if ( is.na( sll ) ) sll = -Inf
  
  return( -sll )
}

# 2.3)
estimate_ed_lm = function( x, y ) {
  # Purpose:
  # Fits an exponential decay model via 
  # tobit log-linear regression.
  # Arguments:
  # x    - The independent variable
  # y    - The dependent variable
  # Returns:
  # A list with the parameter estimates,
  # the predicted THCCOOH levels, 
  # and the residals.
  
  # Straightforward estimation of the 
  # power law and exponential decay models 
  # requires log transformations, so we 
  # first check for zeroes in the dependent 
  # variable
  is_zero = y == 0
  
  # If there are no zero values, we can simply 
  # use a log-linear model to estimate the 
  # 3 parameters in the model
  if ( !any( is_zero ) ) {
    
    lmf = lm( log( y ) ~ 1 + x )
    est = c( coef( lmf ), sigma( lmf ) )
    est[1] = exp( est[1] )
    est[2] = -est[2]
    
  } else {
    
    # If there are zeros, we can treat them 
    # as censored data (i.e., the actual 
    # values are actually non-zero, but 
    # any value less than a cut-off was 
    # rounded down to zero). Therefore, 
    # we can use tobit regression to 
    # fit the log-linear model
    
    # First, we use the naive estimator 
    # to determine the starting values 
    # for the optimization routine
    sel = y > 0
    ne = lm( log( y[sel] ) ~ 1 + x[sel] )
    st_val = c( coef( ne ), sigma( ne ) )
    st_val[1] = exp( st_val[1] )
    st_val[2] = -st_val[2]
    
    # Set up data to analyze
    dat = list(
      x = x,
      y = y,
      a = 1
    )
    # We can then use the tobit regression likelihood 
    # to estimate the parameters of interest despite 
    # the censored data
    tbtf = tryCatch(
      optim( st_val, mle_ed_tobit, dat = dat,
             control = list( maxit = 1000 ) ),
      error = function(e) NULL
    )
    
    # If the MLE worked
    if ( !is.null( tbtf ) ) {
      
      est = tbtf$par
      
    } else {
      est = st_val
      warning( 'Tobit regression failed' )
    }
    
  }
  names( est ) = c( 'Baseline', 'Elimination', 'Error' )
  
  # Generate predicted THCCOOH levels
  pred = ed_model( est, x )
  # Compute residuals
  resid_val = y - pred
  
  return( list( est = est, 
                x = x, 
                pred = pred, 
                resid = resid_val ) )
  
}

# 2.4) 
plot_decay = function( dat, new = T, estimate = T ) {
  # Purpose:
  # Plots the decay in THCCOOH and log THCCOOH levels 
  # over days since ingestion.
  # Arguments:
  # dat      - A data frame with the Time, THCCOOH,
  #            and ID variables. If there are 
  #            multiple subjects, the average 
  #            will also be plotted
  # new      - Logical; if true, a new plotting 
  #            pane is generated
  # estimate - Logical; if true, the 
  #            exponential decay model is fit 
  #            to the data using a tobit regression 
  #            approach.
  
  # Extract data
  x = dat$Time
  y = dat$THCCOOH
  y_log = log( y )
  id = dat$ID
  Ns = length( unique( id ) )
  
  if ( Ns > 1 ) {
    plt = dat %>% 
      group_by( Time ) %>% 
      summarise(
        R = mean( THCCOOH )
      )
    plt2 = dat %>%
      filter( THCCOOH > 0 ) %>% 
      group_by( Time ) %>% 
      summarise(
        R = mean( log( THCCOOH ) )
      )
    
  }
  
  if ( estimate ) {
    est = estimate_ed_lm( x, y )
    o = order( est$x )
    x_est = est$x[o]
    y_hat = est$pred[o]
  }
  
  # Create two panes
  if ( new ) x11( width = 12 )
  layout( cbind( 1, 2 ) )
  
  ### Raw data
  
  # Create a blank plot
  xl = c( -1, 35 )
  yl = lowerUpper( 100, y )
  yl[1] = -7.5
  blankPlot( xl, yl )
  
  # Add observations
  points( x, y, pch = 19, col = 'grey' )
  
  # Add mean
  if ( Ns > 1 ) lines( plt$Time, plt$R, lwd = 2 )
  # Add estimates
  if ( estimate ) lines( x_est, y_hat, lwd = 2, col = 'blue' )
  
  # Add axes and labels
  customAxes( xl, yl )
  
  # x-axis
  axis( 1, seq( 0, 32, 4 ),
        tick = F, line = -1, cex.axis = 1.25 )
  mtext( 'Days since THC ingested',
         side = 1, line = 2, cex = 1.25 )
  
  # y-axis
  axis( 2, round( seq( 0, yl[2], length = 5 ) ),
        tick = F, line = -1, cex.axis = 1.25 )
  mtext( 'THCCOOH - ng/mL',
         side = 2, line = 2, cex = 1.25 )
  
  ### Log-transformed data
  
  # Create a blank plot
  sel = y_log > -Inf & y_log < Inf
  yl = lowerUpper( 1, y_log[sel] )
  blankPlot( xl, yl )
  
  horizLines( log(1), xl, lwd = 2, lty = 2, col = 'grey' )
  
  customAxes( xl, yl,
              label = c( 'Days since THC ingestion', 'THCCOOH - log(ng/mL)' ),
              axSz = 1.25 )
  axis( 1, seq( 0, 32, 4 ),
        tick = F, line = -1, cex.axis = 1.25 )
  axis( 2, round( seq( yl[1], yl[2], length = 5 ), 1 ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  
  points( x, y_log, pch = 19 )
  # Add mean
  if ( Ns > 1 ) lines( plt2$Time, plt2$R, lwd = 2 )
  # Add estimates
  if ( estimate ) {
    sel = log( y_hat ) > yl[1]
    lines( x_est[sel], log( y_hat[sel] ), lwd = 2, col = 'blue' )
  }
  
}

# 2.5) 
estimate_ed_brms = function( dtbf, 
                             covariates,
                             algorithm = 
                               list(
                                 wup = 1000,
                                 d_iter = 10000/4,
                                 thin = 1,
                                 seed = NA ),
                             horseshoe = F ) {
  # Purpose:
  # Fits an exponential decay model to 
  # THCCOOH levels over days with both 
  # population and subject-level effects.
  # Arguments:
  # dtbf       - A data frame with the 
  #              log THCCOOH levels, an 
  #              intercept vector 'Start_point',
  #              days coded as 'Elimination_Rate',
  #              and the standardized predictors
  #              with the tags '.SP' or '.ER' 
  #              to denote with non-linear 
  #              parameter they are associated with
  # covariates - An optional list with two vectors,
  #              'SP', and 'ER', indicating the 
  #              predictors to include for 
  #              the start point and elimination
  #              rate parameters
  # algorithm  - A list with the warmup iterations,
  #              number of desired samples for each 
  #              chain, the thinning value, and 
  #              the seed for the random number 
  #              generator
  # horseshoe  - Logical; if true, instead fits 
  #              a model with all predictors 
  #              included and a horseshoe prior 
  #              to shrink small effects without 
  #              overly impacted larger effects
  # Returns:
  # A list with the brms output,
  # the estimated LOO-CV, the 
  # estimation run time, and 
  # the formula used to fit 
  # the data.
  
  # Estimate run time
  tick()
  
  # Extract settings for estimation 
  # algorithm
  
  # Warm up period
  wup = algorithm$wup
  # Desired number of iterations
  d_iter = algorithm$d_iter
  # Thinning value
  thin = algorithm$thin
  n_iter = d_iter * thin + wup
  seed = algorithm$seed
  
  # If not using a horseshoe prior
  if ( !horseshoe ) {
    
    ### Create estimation formula
    
    # Create brms formula
    lhs = 'log_THCCOOH ~ 0 +'
    
    # Check for additional predictors of 
    # the start point
    if ( !is.null( covariates$SP ) ) {
      
      SP = paste( 'Start_point +',
                  paste( 
                    covariates$SP, 
                    collapse = ' + ' ),
                  '+' )
    } else {
      SP = 'Start_point +'
    }
    
    # Check for additional predictors of 
    # slope
    if ( !is.null( covariates$ER ) ) {
      
      # Create formula
      ER = paste( 'Elimination_rate +',
                  paste( 
                    covariates$ER, 
                    collapse = ' + ' ),
                  '+ ' )
    } else {
      ER = 'Elimination_rate +'
    }
    
    # Subject-level effects
    SLE = '(0 + Start_point + Elimination_rate | ID)'
    
    frm = paste( lhs, SP, ER, SLE )
    
    ### Specify priors
    
    # Weakly informative prior on residual error
    prior = set_prior( 'student_t( 19, .33, .22 )', 
                       class = 'sigma' )
    
    # Prior on starting point
    prior = c( prior,
               set_prior( 'normal( 4.6, 1.1 )', 
                          class = 'b', coef = 'Start_point' ) )
    
    # Prior on elimination rate
    prior = c( prior,
               set_prior( 'student_t( 15, .23, .16 )', 
                          class = 'b', coef = 'Elimination_rate' ) )
    
    # Prior on correlation between random effects
    prior = c( prior,
               set_prior( 'lkj_corr_cholesky(1)', 
                          class = 'L' ) )
    
    if ( !is.null( covariates$SP ) ) {
      # Set mild regularizing priors on predictors
      for ( i in 1:length( covariates$SP ) ) {
        prior = c( prior,
                   set_prior( 'normal( 0, 1 )', 
                              class = 'b', 
                              coef = covariates$SP[i] ) )
      }
    }
    
    if ( !is.null( covariates$ER ) ) {
      # Set mild regularizing priors on predictors
      for ( i in 1:length( covariates$ER ) ) {
        prior = c( prior,
                   set_prior( 'normal( 0, 1 )', 
                              class = 'b', 
                              coef = covariates$ER[i] ) )
      }
    }
    
    # Fit model using brms
    mf = brm( as.formula( frm ),
              # Data and priors
              data = dtbf, prior = prior, 
              # Estimation settings
              warmup = wup,
              iter = n_iter,
              chains = 4,
              cores = 4,
              thin = thin, 
              control = list( adapt_delta = 0.995,
                              max_treedepth = 15 ) )
    
    
    
  } else {
    
    ### Horshoe prior example
    
    # Weakly informative prior on residual error
    prior = set_prior( 'student_t( 10, .33, .5 )', 
                       class = 'sigma' )
    # Prior on starting point
    prior = c( prior,
               set_prior( 'normal( 4.6, 2.5 )', 
                          class = 'b', coef = 'Start_point',
                          nlpar = 'eta1' ) )
    
    # Prior on elimination rate
    prior = c( prior,
               set_prior( 'student_t(15, .23, .16 )', 
                          class = 'b', coef = 'Elimination_rate',
                          nlpar = 'eta1' ) )
    
    # Prior on correlation between random effects
    prior = c( prior,
               set_prior( 'lkj_corr_cholesky(1)', 
                          class = 'L' ) )
    
    # Set a horseshoe prior on predictors to shrink 
    # small effects and increase large effects
    prior = c( prior,
               prior( horseshoe(1), nlpar = 'eta2' ) )
    
    frm = bf( log_THCCOOH ~ eta1 + eta2,
              eta1 ~ 0 + Start_point + Elimination_rate + 
                (0 + Start_point + Elimination_rate|ID),
              eta2 ~ 0 + 
                # Predictors for start point
                Sex.SP + 
                zBMI.SP + 
                zYears_of_MJ_use.SP + 
                zLevel_of_MJ_use.SP + 
                Race.SP + 
                # Predictors for elimination rate
                Sex.ER + 
                zBMI.ER + 
                zYears_of_MJ_use.ER + 
                zLevel_of_MJ_use.ER + 
                Race.ER,
              nl = TRUE )
    
    mf = brm( frm, 
              # Data and priors
              data = dtbf, prior = prior, 
              # Estimation settings
              warmup = wup,
              iter = n_iter,
              chains = 4,
              cores = 4,
              thin = thin, 
              seed = seed, 
              control = list( adapt_delta = 0.9999,
                              max_treedepth = 15 ) )
    
  }
  
  # Compute approximation to leave-one-out
  # cross-validation (LOOCV)
  loocv = loo::loo( mf )
  
  tock()
  
  out = list(
    model_fit = mf,
    loocv = loocv,
    run_time = run_time,
    formula = frm
  )
  
  return( out )
}

###
### 3) Plot of THCCOOH by day
###

if ( run_code[1] ) {
  
  plot_decay( cd )
  legend( 'topright',
          c( 'Observed', 'Mean', 'Censored-corrected predicted' ),
          fill = c( 'grey', 'black', 'blue' ),
          bty = 'n', cex = 1.25 )
  title( paste( round( 100 * sum( cd$THCCOOH == 0 )/nrow(cd) ),
                '% total obs. equal 0', sep = '' ) )
  
  # Summarize data issues
  di = all_dat %>% 
    group_by( ID ) %>% 
    summarize( 
      Nu = sum( Data_issues == '0' ), 
      DI = unique( Data_issues )[ which.max( nchar( unique( Data_issues ) ) ) ],
      Nz = sum( THCCOOH == 0, na.rm = T )
    )
  # Identify subjects removed from the study
  di = di[ di$Nu <= 1 | di$Nz >= 6, ]
  di = di %>% arrange( DI )
  
  di = data.frame(
    Count = 1:nrow( di ), 
    Excluded_subjects = di$ID,
    Issue = c(
    rep( 'Only one observation with detectable levels', 3 ),
    rep( 'Did not use THC recently', 10 ),
    rep( 'Six or more missing observations', 3 ),
    'Did not maintain abstinence' ),
    stringsAsFactors = F
  )
  setwd( dat_dir )
  setwd( 'Original_files' )
  write.table( di,
               row.names = F,
               quote = F,
               sep = ',',
               file = 'Excluded_subjects.csv'
  )
  setwd( R_dir )
  
  # Average number of days since baseline measurement 
  # for each visit
  b('
  dtbf %>% 
    group_by( Visit_number ) %>% 
    summarise( M = round( mean( Days_from_baseline ) ) )
  ')
  
}

###
### 4) Data imputation based on tobit regression
###

if ( run_code[2] ) {
  
  # Determine subjects with sufficient data
  check = cd %>% 
    group_by( ID ) %>% 
    summarize(
      # Number of observations
      No = length( ID ),
      # Number of zeros
      Nz = sum( THCCOOH == 0 )
    )
  # Non-zero values
  check$Nnz = 
    check$No - check$Nz
  check$Nog1 = 
    check$No > 1
  check$Nnzg1 = 
    check$Nnz > 1
  
  # Take subjects with 2 or more non-zero observations
  dtbf = cd %>% 
    filter( ID %in% check$ID[ check$Nog1 & check$Nnzg1 ] )
  subj = unique( dtbf$ID )
  Ns = length( subj )
  
  all_est = matrix( NA, Ns, 3 )
  all_output = c()
  for ( s in 1:Ns ) all_output = c( all_output, list(NULL) )
  
  # Measure run time
  tick()
  # Initialize progress bar
  pb = txtProgressBar( min = 0, max = Ns, style = 3 )
  for ( s in 1:Ns ) {
    
    sel = dtbf$ID == subj[s]
    est = estimate_ed_lm( dtbf$Time[sel],
                          dtbf$THCCOOH[sel] )
    chk = sum( dtbf$THCCOOH[sel] > 0 )
    if ( chk == 2 ) est$est[3] = 0.0
    all_est[s,] = est$est
    all_output[[s]] = list( est )
    
    # Update the progress bar
    setTxtProgressBar(pb,s)
  }
  close(pb)
  tock()
  print( run_time )
  
  # Data imputation for zeros
  for ( s in 1:Ns ) {
    sel = dtbf$ID == subj[s]
    is_zero = dtbf$THCCOOH[sel] == 0
    if ( any( is_zero ) ) {
      dtbf$THCCOOH[sel][is_zero] = 
        all_output[[s]][[1]][['pred']][is_zero]
    }
  }
  
  # Convert to data frame
  all_est = data.frame(
    ID = subj,
    Start_point = all_est[,1],
    Log_start_point = log( all_est[,1] ), 
    Elimination_rate = all_est[,2],
    Residual_SD = all_est[,3],
    stringsAsFactors = F
  )
  
  ### Set up primary variables
  
  dtbf$log_THCCOOH = 
    log( dtbf$THCCOOH )
  dtbf$Start_point = 1
  dtbf$Elimination_rate = -dtbf$Time
  
  ### Set up predictors for start point
  
  dtbf$Sex.SP = dtbf$Sex
  
  # Standardize predictors
  dtbf$zBMI.SP = 
    my_standardize( dtbf$BMI )
  dtbf$zYears_of_MJ_use.SP = 
    my_standardize( dtbf$Years_of_MJ_use )
  dtbf$zLevel_of_MJ_use.SP = 
    my_standardize( dtbf$Level_of_MJ_use )
  
  ### Set up predictors for elimination rate
  
  dtbf$Sex.ER = dtbf$Sex * dtbf$Time
  
  # Standardize predictors
  dtbf$zBMI.ER = 
    dtbf$zBMI * dtbf$Time
  dtbf$zYears_of_MJ_use.ER = 
    dtbf$zYears_of_MJ_use * dtbf$Time
  dtbf$zLevel_of_MJ_use.ER = 
    dtbf$zLevel_of_MJ_use * dtbf$Time
  
  # Create table with the raw versus data imputation values
  tmp = cd[ cd$ID %in% dtbf$ID, ]
  tbl = tmp %>% 
    arrange( ID, Time ) %>% 
    select( ID, Time, Visit_number, THCCOOH )
  tmp2 = dtbf %>% arrange( ID, Time )
  # Check that ID and time variables are aligned correctly
  # all( tbl$ID == tmp2$ID & tbl$Time == tmp2$Time )
  tbl$Tobit = tmp2$THCCOOH
  colnames( tbl ) = c( 'ID', 'Days since last use', 'Visit', 'Original', 'Tobit' )
  
  # Add in rows for subjects 10028 and 10076
  tmp = tbl[1:2,]
  tmp$ID = c( '10028', '10076' )
  tmp$Visit = c( 4, 6 )
  for ( i in 1:2 ) {
    sel = all_dat$ID == tmp$ID[i] & 
      all_dat$Visit_number == tmp$Visit[i]
    tmp$`Days since last use`[i]= all_dat$Time[sel] + 
      all_dat$Recency_of_MJ_use[sel]
    tmp$Original[i] = all_dat$THCCOOH[sel]
  }
  sel = all_est$ID %in% tmp$ID
  tmp$Tobit = exp( all_est$Log_start_point[sel] - 
                     all_est$Elimination_rate[sel] * tmp$`Days since last use` )
  tmp$Tobit[2] = tmp$Original[2]
  tbl = rbind( tbl, tmp )
  
  # Sort data
  tbl = tbl %>% 
    arrange( ID, Visit )
  
  tbl$Tobit = round( tbl$Tobit, 1 )
  tbl$Original = round( tbl$Original, 1 )
  sel = tbl$Original == 0
  # Note which values were censored
  tbl$Original[sel] = ' < 5'

  # Save results
  setwd( proj_dir )
  setwd( 'Documents' )
  write.table( tbl,
               row.names = F,
               quote = F,
               sep = ',',
               file = 'Imputed_values.csv' )
  setwd( R_dir )
  
  # Clean up workspace
  rm( tmp, tbl, sel, tmp2 )
  
}

###
### 5) Bayesian population-level model (example)
###

if ( all( run_code[2:3] ) ) {
  
  ### Specify empirical bayes priors for start point/elimination rate
  
  ebp = all_est %>% 
    summarize(
      M_LSP = mean( Log_start_point ),
      SD_LSP = sd( Log_start_point ),
      SEM_LSP = sem( Log_start_point ),
      M_ER = mean( Elimination_rate ),
      SD_ER = sd( Elimination_rate ),
      SEM_ER = sem( Elimination_rate ),
      M_RSD = mean( Residual_SD ),
      SD_RSD = sd( Residual_SD ),
      SEM_RSD = sem( Residual_SD )
    )
  # round( ebp, 1 )
  
  ### Basic regression example
  
  # Weakly informative prior on residual error
  prior = set_prior( 'student_t( 19, .33, .22 )', 
                     class = 'sigma' )
  
  # Prior on starting point
  prior = c( prior,
             set_prior( 'normal( 4.6, 1.1 )', 
                        class = 'b', coef = 'Start_point' ) )
  
  # Prior on elimination rate
  prior = c( prior,
             set_prior( 'student_t( 15, .23, .16 )', 
                        class = 'b', coef = 'Elimination_rate' ) )
  
  # Prior on correlation between random effects
  prior = c( prior,
             set_prior( 'lkj_corr_cholesky(1)', 
                        class = 'L' ) )
  
  # Warm up period
  wup = 1000
  # Desired number of iterations
  d_iter = 10000/4
  # Thinning value
  thin = 5
  n_iter = d_iter * 5 + wup
  
  tick()
  m0 = brm( log_THCCOOH ~ 0 + # Remove standard intercept
              # Population effects (Start point)
              Start_point + 
              # Population effects (Elimination rate)
              Elimination_rate +
              # Subject-level effects
              ( 0 + Start_point + Elimination_rate | ID ),
            # Data and priors
            data = dtbf, prior = prior, 
            # Estimation settings
            warmup = wup,
            iter = n_iter,
            chains = 4,
            cores = 4,
            thin = thin, 
            control = list( adapt_delta = 0.995,
                            max_treedepth = 15 ) )
  tock()
  print( run_time )
  
  tick()
  x11(); pp_check( m0 )
  
  x11(); plot( marginal_effects( m0 ), points = T )
  
  for ( i in 1:ceiling( Ns/9 ) ) {
    
    x11()
    ind = 1:9 + 9 * (i-1)
    if ( max( ind ) > Ns ) ind = min(ind):Ns
    conditions = data.frame( ID = unique(dtbf$ID)[ind], stringsAsFactors = F )
    rownames(conditions) = unique(dtbf$ID)[ind]
    Fit_by_subject = marginal_effects( m0, conditions = conditions,
                                       re_formula = NULL, method = "predict")
    plot( Fit_by_subject, nrow = 3, ncol = 3, points = TRUE)
    
  }
  tock()
  print( run_time )
  
  ### Horshoe prior example
  
  # Weakly informative prior on residual error
  prior = set_prior( 'student_t( 19, .33, .22 )', 
                     class = 'sigma' )
  
  # Prior on starting point
  prior = c( prior,
             set_prior( 'normal( 4.6, 1.1 )', 
                        class = 'b', coef = 'Start_point',
                        nlpar = 'eta1' ) )
  
  # Prior on elimination rate
  prior = c( prior,
             set_prior( 'student_t( 15, .23, .16 )', 
                        class = 'b', coef = 'Elimination_rate',
                        nlpar = 'eta1' ) )
  
  # Prior on correlation between random effects
  prior = c( prior,
             set_prior( 'lkj_corr_cholesky(1)', 
                        class = 'L' ) )
  
  # Horseshoe prior on remaining coefficients
  prior = c( prior,
             prior( horseshoe(1), nlpar = 'eta2' ) )
  
  bformula = bf( log_THCCOOH ~ eta1 + eta2,
                 eta1 ~ 0 + Start_point + Elimination_rate + 
                   (0 + Start_point + Elimination_rate|ID),
                 eta2 ~ 0 + Sex.SP + zBMI.SP + zYears_of_MJ_use.SP + 
                   zLevel_of_MJ_use.SP + Sex.ER + 
                   zBMI.ER + zYears_of_MJ_use.ER + 
                   zLevel_of_MJ_use.ER,
                 nl = TRUE)
  
  # Warm up period
  wup = 1000
  # Desired number of iterations
  d_iter = 10000/4
  # Thinning value
  thin = 1
  n_iter = d_iter * thin + wup
  
  m1 = brm( bformula, 
            # Data and priors
            data = dtbf, prior = prior, 
            # Estimation settings
            warmup = wup,
            iter = n_iter,
            chains = 4,
            cores = 4,
            thin = thin, 
            control = list( adapt_delta = 0.995,
                            max_treedepth = 15 ) )
  
}

###
### 6) Nested cross-validation approach
###

# 6.1)
extract_brm_res = function( mf ) {
  # Purpose:
  # A function to extract the estimated 
  # effect size, posterior standard deviation,
  # 95% credible interval, and posterior 
  # p-value for the regression coefficients,
  # subject-level variances, and measurement 
  # error variability.
  # Arguments:
  # mf - The brms output object
  # Returns:
  # A matrix with the effect size, 
  # posterior standard deviation, 
  # 95% credible intervals, and 
  # one-sided posterior p-values for 
  # a subset of parameters.
  
  # Function to compute posterior p-values
  pv = function( x ) {
    if ( mean(x) > 0 ) {
      p = sum( x < 0 )/length( x )
    } else {
      p = sum( x > 0 )/length( x )
    }
    return( p )
  }
  
  # Extract summary of results
  sm = summary( mf )
  
  # Combine results of interest
  res = rbind(
    sm$fixed[,1:4],
    sm$random$ID[,1:4],
    Measurement_error = sm$spec_pars[,1:4]
  )
  colnames( res ) = c(
    'Effect_size',
    'SD',
    'Lower_CI_95',
    'Upper_CI_95'
  )
  
  # Compute posterior p-values
  out = cbind( res, p_value = NA )
  
  # Extract posterior samples
  pst = as.matrix( mf )
  # Compute posterior p-values
  out[,'p_value'] = apply( pst[,1:nrow(out)], 2, pv )
  
  out = data.frame(
    Variable = c(
      rownames( sm$fixed ),
      'sd(Start_point)',
      'sd(Elimination_rate)',
      'cor(Start_point,Elimination_rate)',
      'Measurement_error'
    ),
    X = out,
    stringsAsFactors = F
  )
  
  return( out )
}

# 6.2)
compute_coverage_prob = function( mf, test ) {
  # Purpose:
  # Computes the proportion of observations 
  # that fall within the 95% credible 
  # prediction intervals.
  # Arguments:
  # mf   - A brms output object
  # test - A data frame with the holdout observations
  # Returns:
  # The proportion of observations that fell 
  # within the 95% credible prediction intervals.
  
  # Generate predictions based on 
  # holdout sample
  pred = predict( mf$model_fit, 
                  newdata = test, allow_new_levels = T )
  # Compute coverage probabilities
  coverage_prob = 
    sum( pred[,3] < test$log_THCCOOH & 
           pred[,4] > test$log_THCCOOH )/nrow( test )
  return( coverage_prob )
}

# 6.3)
compute_R2 = function( mf, test ) {
  # Purpose:
  # Computes the R-squared coefficient 
  # between the model predictions and the 
  # observed data for a holdout test 
  # sample.
  # Arguments:
  # mf   - A brms output object
  # test - A data frame with the holdout observations
  # Returns:
  # The R-squared coefficient, its standard deviation, 
  # and its 95% credible interval.
  
  pred = predict( mf$model_fit, 
                  newdata = test, 
                  allow_new_levels = T,
                  summary = F )
  all_r2 = apply( pred, 1, function(x) cor( x, test$log_THCCOOH )^2 )
  
  out = c(
    R2 = mean( all_r2 ),
    SD = sd( all_r2 ),
    CI_2.5 = quantile( all_r2, .025 ),
    CI_97.5 = quantile( all_r2, .975 )
  )
  
  return( out )
}

# 6.4) Fit models to data
if ( all( run_code[c(2,4)] ) ) {
  
  # Specify seeds for randomization 
  # for reproducibility
  rng_seeds = c(
    Splits = 2697,
    F1 = 9832,
    F2 = 7849,
    F3 = 1623,
    F4 = 1129
  )
  
  # 2697
  
  ### Set up primary variables
  
  dtbf$log_THCCOOH = 
    log( dtbf$THCCOOH )
  dtbf$Start_point = 1
  dtbf$Elimination_rate = -dtbf$Time
  
  ### Set up predictors for start point
  
  dtbf$Sex.SP = dtbf$Sex
  dtbf$Nicotine_use.SP = dtbf$Nicotine_use
  dtbf$Race.SP = dtbf$Race
  
  # Standardize predictors
  dtbf$zBMI.SP = 
    my_standardize( dtbf$BMI )
  dtbf$zYears_of_MJ_use.SP = 
    my_standardize( dtbf$Years_of_MJ_use )
  dtbf$zLevel_of_MJ_use.SP = 
    my_standardize( dtbf$Level_of_MJ_use )
  dtbf$zCUDIT.SP = 
    my_standardize( dtbf$CUDIT.SS )
  
  ### Set up predictors for elimination rate
  
  dtbf$Sex.ER = dtbf$Sex * dtbf$Time
  dtbf$Nicotine_use.ER = dtbf$Nicotine_use * dtbf$Time
  dtbf$Race.ER = dtbf$Race * dtbf$Time
  
  # Standardize predictors
  dtbf$zBMI.ER = 
    dtbf$zBMI.SP * dtbf$Time
  dtbf$zYears_of_MJ_use.ER = 
    dtbf$zYears_of_MJ_use.SP * dtbf$Time
  dtbf$zLevel_of_MJ_use.ER = 
    dtbf$zLevel_of_MJ_use.SP * dtbf$Time
  dtbf$zCUDIT.ER = 
    dtbf$zCUDIT.SP * dtbf$Time
  
  # Outer fold uses 4-fold cross validation
  K = 4
  # Use function from R package 'loo' to 
  # randomly split subjects into 
  # 4 subsets
  subj = unique( dtbf$ID )
  set.seed( rng_seeds[1] ) # For reproducibility
  four_fold = loo::kfold_split_random( 
    K, 
    N = length( unique( dtbf$ID ) ) )
  
  # Initialize output for each fold
  output = c()
  for ( k in 1:K ) output = c( output, list(NULL) )
  names( output ) = paste( 'Fold', 1:K, sep = '_' )
  
  # Loop over subsets
  for ( k in 1:K ) {
    
    # Initialize lists to store details 
    # for model fit
    coverage_prob = list(
      m0 = NULL,
      m1 = NULL,
      m2 = NULL
    )
    
    R2_values = list(
      m0 = NULL,
      m1 = NULL,
      m2 = NULL
    )
    
    # Specify training and test data
    test_sel = subj[ four_fold == k ]
    test = dtbf[ dtbf$ID %in% test_sel, ]
    train_sel = subj[ four_fold != k ]
    train = dtbf[ dtbf$ID %in% train_sel, ]
    
    # Estimation settings
    algorithm = list(
      wup = 1000,
      d_iter = 10000/4,
      thin = 1,
      seed = rng_seeds[k+1]
    )
    
    # Fit models to training data
    
    ### Model 0 - Null model
    
    m0 = estimate_ed_brms( train, list(),
                           algorithm = algorithm )
    
    # Extract and save results
    res0 = extract_brm_res( m0$model_fit )
    coverage_prob$m0 = compute_coverage_prob( m0, test )
    R2_values$m0 = compute_R2( m0, test )
    
    ### Model 1 - All predictors with horseshoe prior
    
    cvrts = list(
      SP = NULL,
      ER = NULL
    )
    m1 = estimate_ed_brms( train, cvrts,
                           algorithm = algorithm,
                           horseshoe = T )
    
    # Extract and save results
    res1 = extract_brm_res( m1$model_fit )
    coverage_prob$m1 = compute_coverage_prob( m1, test )
    R2_values$m1 = compute_R2( m1, test )
    
    # Determine significant coefficients
    val = which( 
      res1$Variable %in% c( 'eta1_Start_point', 
                            'eta1_Elimination_rate',
                            'sd(Start_point)',
                            'sd(Elimination_rate)', 
                            'cor(Start_point,Elimination_rate)',
                            'Measurement_error' ) )
    
    # Extract significant predictors
    # Sampling error around .05
    # qbinom( c( .025, .975 ), 10000, .05 )/10000
    which_sig = which( res1$X.p_value[-val] < .0543 )
    
    ### Model 2 - Significant predictors only
    
    if ( length( which_sig ) > 0 ) {
      
      # Determine which predictors should be included
      sig_pred = res1$Variable[-val][ which_sig ]
      sig_pred = unlist( strsplit( sig_pred, split = 'eta2_' ) )
      sig_pred = sig_pred[ sig_pred != '' ]
      
      # Fit model with significant predictors only
      cvrts = list(
        SP = NULL,
        ER = NULL
      )
      cvrts$SP = sig_pred[ grep( 'SP', sig_pred ) ]
      cvrts$ER = sig_pred[ grep( 'ER', sig_pred ) ]
      if ( length( cvrts$SP ) == 0 ) cvrts$SP = NULL
      if ( length( cvrts$ER ) == 0 ) cvrts$ER = NULL
      
      m2 = estimate_ed_brms( train, cvrts,
                             algorithm = algorithm )
      
      # Extract and save results
      res2 = extract_brm_res( m2$model_fit )
      coverage_prob$m2 = compute_coverage_prob( m2, test )
      R2_values$m2 = compute_R2( m2, test )
      
      ### Cross-validation for inner fold
      
      # Determine the model with the best LOO-CV
      model_comp = loo::loo_model_weights( list( 
        m0 = m0$loocv, 
        m1 = m1$loocv, 
        m2 = m2$loocv ) )
      
      # Combined results
      all_res = data.frame(
        Model = c( rep( 'Model_0', nrow( res0 ) ), 
                   rep( 'Model_1', nrow( res1 ) ), 
                   rep( 'Model_2', nrow( res2 ) ) ),
        stringsAsFactors = F
      )
      all_res = cbind( all_res, rbind( res0, res1, res2 ) )
      
    } else {
      
      # Remove last model
      coverage_prob$m2 = NULL
      
      ### Cross-validation for inner fold
      
      # Determine the model with the best LOO-CV
      model_comp = loo::loo_model_weights( list( 
        m0 = m0$loocv, 
        m1 = m1$loocv ) )
      
      # Combine results
      all_res = data.frame(
        Model = c( rep( 'Model_0', nrow( res0 ) ), 
                   rep( 'Model_1', nrow( res1 ) ) ),
        stringsAsFactors = F
      )
      all_res = cbind( all_res, rbind( res0, res1 ) )
      
    }
    
    # Save results for subset
    output[[k]] = list(
      results = all_res,
      CV_coverage_prob = coverage_prob,
      R_squared = R2_values,
      model_comparisons = model_comp
    )
    
  }
  
  # Save results
  setwd( dat_dir )
  save( output, dtbf, 
        file = '4_fold_cross_validation_results.RData' )
  setwd( R_dir )
}

###
### 7) Best-fitting model
###

if ( run_code[5] ) {
  
  # Load in previous results
  setwd( dat_dir )
  load( '4_fold_cross_validation_results.RData' )
  setwd( R_dir )
  
  # Which model fit the best on average
  K = length( output )
  param = matrix( NA, K, 3 )
  which_coef = c()
  for ( k in 1:K ) {
    sel = which.max( as.numeric(  output[[k]]$model_comparisons ) )
    param[k,1] = sel - 1
    param[k,2] = unlist( output[[k]]$CV_coverage_prob[sel] )
    param[k,3] = unlist( output[[k]]$R_squared[sel][1] )[1]
    
    # Determine coefficients included in model 2
    sel = output[[k]]$results$Model == 'Model_2'
    vrb = output[[k]]$results$Variable[sel]
    which_coef = c( which_coef, 
                    list( 
                      vrb[ c( grep( '.SP', vrb ), grep( '.ER', vrb ) ) ] ) )
  }
  # Display results
  # round( colMeans(param), 2 )
  
  # Best-fitting model
  # In all folds, level of MJ use significantly predicted 
  # start point, and 3 of the 4 folds, was preferred 
  # over the full and null models
  cvrts = list(
    SP = 'zLevel_of_MJ_use.SP',
    ER = NULL
  )
  
  # Estimation settings
  algorithm = list(
    wup = 1000,
    d_iter = 10000/4,
    thin = 1,
    seed = 2991
  )
  
  # Estimate model
  bfm = estimate_ed_brms( dtbf, cvrts,
                          algorithm = algorithm )
  
  # Check subject-level fits
  setwd( fig_dir )
  pdf( 'Individual_fits.pdf' )
  for ( i in 1:ceiling( Ns/9 ) ) {
    
    # x11()
    ind = 1:9 + 9 * (i-1)
    if ( max( ind ) > Ns ) ind = min(ind):Ns
    conditions = data.frame( ID = unique(dtbf$ID)[ind], stringsAsFactors = F )
    rownames(conditions) = unique(dtbf$ID)[ind]
    Fit_by_subject = marginal_effects( bfm$model_fit, 
                                       effect = 'Elimination_rate', 
                                       conditions = conditions,
                                       re_formula = NULL, method = "predict")
    plot( Fit_by_subject, nrow = 3, ncol = 3, points = TRUE)
    
  }
  dev.off()
  
  # Model with all predictors
  
  # Estimation settings
  algorithm_fm = list(
    wup = 1000,
    d_iter = 10000/4,
    thin = 1,
    seed = 9348
  )
  
  # Use horseshoe prior
  cvrts_fm = list(
    SP = NULL,
    ER = NULL
  )
  fm = estimate_ed_brms( dtbf, cvrts_fm,
                         algorithm = algorithm_fm,
                         horseshoe = T )
  
  # Save posterior estimates
  setwd( dat_dir )
  save( bfm, fm, dtbf, param, which_coef, algorithm, cvrts,
        file = 'Best_fitting_model.RData' )
  setwd( R_dir )
  
}

###
### 8) Model results
###

if ( run_code[6] ) {
  
  # 8.1) Functions for summarizing results
  
  # Mean for a log-normal distribution
  ln_m = function( mu, sigma ) {
    s2 = pow( sigma, 2 )
    out = exp( mu + s2/2 )
    return( out )
  }
  
  # Standard deviation for a log-normal distribution
  ln_sd = function( mu, sigma ) {
    s2 = pow( sigma, 2 )
    out = ( exp( s2 ) - 1 ) * exp( 2*mu + s2 )
    return( sqrt( out ) )
  }
  
  # Function to compute posterior p-values
  pv = function( x ) {
    if ( mean(x) > 0 ) out = sum( x < 0 )/length(x)
    if ( mean(x) <= 0 ) out = sum( x > 0 )/length(x)
    return( out )
  }
  
  # 8.2) Load in results
  
  setwd( dat_dir )
  load( file = 'Best_fitting_model.RData' )
  setwd( R_dir )
  
  # Summarize results
  b('
  # Best-fitting model
  cur_res = extract_brm_res( bfm$model_fit )
  row.names( cur_res ) = 1:nrow( cur_res )
  cur_res = cbind( cur_res[,1], 
                round( cur_res[,c(-1,-6)], 2 ),
                round( cur_res[,6], 3 ) )
  colnames( cur_res ) = c(
    "Variable",
    "Effect_size", "SD", "Lower_CI_95", "Upper_CI_95",
    "p_value" )
  print( cur_res )
  
  # Full model
  cur_res = extract_brm_res( fm$model_fit )
  row.names( cur_res ) = 1:nrow( cur_res )
    cur_res = cbind( cur_res[,1], 
    round( cur_res[,c(-1,-6)], 2 ),
    round( cur_res[,6], 3 ) )
    colnames( cur_res ) = c(
    "Variable",
    "Effect_size", "SD", "Lower_CI_95", "Upper_CI_95",
    "p_value" )
    print( cur_res )
  ')
  
  # Extract posterior estimates for population-level 
  # parameters and subject-level variability
  post = as.matrix( bfm$model_fit )
  
  # Extract columns for subject-level posteriors
  pn = colnames( post )
  sp_subj = grep( 'Start_point', pn )[ -(1:3) ]
  er_subj = grep( 'Elimination_rate', pn )[ -(1:3) ]
  
  # Subject-level elimination rates
  er = colMeans( post[,'b_Elimination_rate'] + post[,er_subj] )
  # Subject-level start points
  sp = colMeans( post[,'b_Start_point'] + post[,sp_subj] )
  
  # Initialize list for result summaries
  all_summary_results = list()
  
  # 8.3) Half-life
  
  # Half-life = log(2) / Elimination_rate
  all_summary_results$Half_life = list(
    # Average half-life
      Mean = round( mean( log(2)/post[,'b_Elimination_rate'] ), 1 ),
      # Standard deviations based on subject-level half-lives
      SD = round( sd( log(2)/er ), 1 ),
      # Range
      Range = round( range( log(2)/er ) )
    )
  
  # 8.4) Window of detection
  
  # Limit of quantitation = 5 ng/mL
  # log(5) = log( a ) - b * x
  # Window of detection = ( log(5) - log( a ) )/-b
  all_summary_results$Window_of_detection = list(
    Mean = round( mean( ( log(5) - post[,'b_Start_point'] ) / 
                   -post[,'b_Elimination_rate'] ) ),
    CI_95 = round( quantile( ( log(5) - post[,'b_Start_point'] ) / 
                           -post[,'b_Elimination_rate'], c( .025, .975 ) ) ),
    Range = round( range( ( log(5) - sp )/-er ) )
  )
  
  # 8.5) Starting level of CN-THCCOOH 
  
  all_summary_results$Starting_level = list(
    Mean = round( mean( ln_m( post[,'b_Start_point'],
                        post[,'sd_ID__Start_point'] ) ) ),
    SD = round( mean( ln_sd( post[,'b_Start_point'],
                             post[,'sd_ID__Start_point'] ) ) ),
    Range = round( range( apply(  post[,'b_Start_point'] + 
                                    post[,sp_subj], 2, function(x) 
      mean( ln_m( x, post[,'sd_ID__Start_point'] ) ) ) ) )
  )
  
  # 8.6) Correlations between dip-stick test and PK parameters
  
  # Load in dip-stick test results per visit
  setwd( dat_dir )
  setwd( 'Original_files' )
  qd = read.csv(
    file = 'Qualitative THC Dataset.csv',
    header = T,
    stringsAsFactors = F)
  # Add day, starting point estimates, and 
  # elimination rate to results
  subjects = unique( qd$id )
  tmp = all_dat[ all_dat$ID %in% subjects, ]
  tmp = tmp %>% arrange( ID, Visit_number )
  qd$Time = NA
  qd$SP = NA
  qd$ER = NA
  for ( i in 1:nrow( qd ) ) {
    sel = tmp$ID == qd$id[i] & 
      tmp$Visit_number == qd$visit_number[i]
    qd$Time[i] = tmp$Time[sel]
    sel = grep( qd$id[i], names( sp ) )
    qd$SP[i] = sp[sel]
    qd$ER[i] = er[sel]
  }
  # Function to compute the last day with a positive result 
  # and the first day with a negative result
  f = function( Test, Time, positive = T ) {
    
    # Initialize output
    out = NA
    
    # Check for missing values
    no_na = !is.na( Test ) & !is.na( Time )
    if ( any( no_na ) ) {
      
      # Last day with positive result
      if ( positive ) {
        
        pos = which( Test[no_na] == 'POS' )
        if ( length( pos ) > 0 ) {
          pos = max( pos )
          out = Time[no_na][ pos ]
          
        }
      # First day with negative result
      } else {
        
        pos = which( Test[no_na] == 'NEG' )
        if ( length( pos ) > 0 ) {
          pos = min( pos )
          out = Time[no_na][ pos ]
        }
        
      }
      
    }
    
    return( out )
  }
  
  # Compute statistics per subject
  val = qd %>% 
    group_by( id ) %>% 
    summarize(
      LPD = f( thc_qual_1, Time ),
      FND = f( thc_qual_1, Time, positive = F ),
      SP = unique( SP ),
      ER = unique( ER )
    )
  
  # Define function to compute posterior p-values 
  # for bivariate correlation
  my_cor = function( x, y ) {
    
    no_na = !is.na( x ) & !is.na( y )
    R = correlationBF( x[no_na], y[no_na] )
    post = posterior( R, iterations = 10000 )
    
    out = list(
      R = mean( post ),
      p.value = mean( post < 0 )
    )
    
    return( out )
  }
  
  DV = c( 'LPD', 'FND' )
  IV = c( 'SP', 'ER' )
  
  cor_results = data.frame(
    DV = c( rep( DV, each = 2 ), DV[1] ),
    IV = c( rep( IV, 2 ), DV[2] ),
    R = NA,
    p_val = NA,
    stringsAsFactors = F
  )
  
  inc = 1
  for ( dv in 1:length( DV ) ) {
    for ( iv in 1:length( IV ) ) {
      tst = my_cor( val[[IV[iv]]], val[[DV[dv]]] )
      cor_results$R[inc] = round( tst$R, 2 )
      cor_results$p_val[inc] = round( tst$p.value, 3 )
      if ( cor_results$p_val[inc] > .5 ) {
        cor_results$p_val[inc] = 1 - 
          cor_results$p_val[inc]
      }
      inc = inc + 1
    }
  }
  tst = my_cor( val[[DV[1]]], val[[DV[2]]] )
  cor_results$R[inc] = round( tst$R, 2 )
  cor_results$p_val[inc] = round( tst$p.value, 3 )
  
  ### Figures
  
  setwd( fig_dir )
  
  # 8.7) Figure 1
  
  png( 'Figure_1.png', width = 480*2, height = 480 )
  
  tmp = names( sp )
  tmp = strsplit( tmp, split = ',' )
  tmp = unlist( lapply( tmp, function(x) x[1] ) )
  tmp = as.character( 
    sapply( tmp, function(x) strsplit( x, 
                                       split = 'r_ID\\[' )[[1]][2] ) )
  check = data.frame(
    ID = tmp,
    log_SP = sp,
    SP = exp( sp ), 
    ER = er,
    stringsAsFactors = F
  )
  rownames( check ) = 1:nrow( check )
  check = check %>% arrange( SP )
  check$N = NA
  check$U = NA
  for ( s in 1:nrow( check ) ) {
    sel = dtbf$ID == check$ID[s]
    check$N[s] = sum(sel)
    check$U[s] = sum( dtbf$THCCOOH[sel] <= 1 )/check$N[s]
  }
  
  # Save model estimates of starting level
  tbl = data.frame(
    ID = check$ID,
    Starting_level = round( 
      ln_m( check$log_SP, mean( post[,'sd_ID__Start_point'] ) ) ),
    stringsAsFactors = F
  )
  setwd( proj_dir )
  setwd( 'Documents' )
  write.table( tbl, 
               row.names = F,
               quote = F,
               sep = ',',
               file = 'Starting_levels.csv'
  )
  setwd( fig_dir )
  
  # Select example subjects
  ex_subj = c(
    # Low starting levels
    '10055',
    '10022',
    # High starting levels
    '10066',
    '10020'
  )
  
  # x11( width = 12 )
  layout( cbind( 1, 2 ) )
  
  xl = c( 0, 35 )
  yl = lowerUpper( 50, dtbf$THCCOOH[ dtbf$ID %in% ex_subj ] )
  yl[1] = -5
  blankPlot( xl, yl )
  
  horizLines( seq( 0, yl[2], 50 ), xl, col = 'grey' )
  
  pts = c( 24, 22, 23, 25 )
  clr = rep( c( 'black', 'white' ), each = 2 )
  
  for ( i in 1:length(ex_subj) ) {
    
    sel = dtbf$ID == ex_subj[i]
    xa = dtbf$Time[ sel ]
    ya = dtbf$THCCOOH[ sel ]
    
    lines( xa, ya, lwd = 2 )
    points( xa, ya, pch = pts[i], bg = clr[i] )
    
  }
  
  # Axes and labels
  customAxes( xl, yl )
  
  # x-axis
  axis( 1, seq( 0, 32, 4 ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  
  # y-axis
  axis( 2, round( seq( 0, yl[2], 50 ) ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'CN-THCCOOH - ng/mg',
         side = 2, line = 2, cex = 1.25 )
  
  yl = lowerUpper( 2.5, dtbf$log_THCCOOH[ dtbf$ID %in% ex_subj ] )
  blankPlot( xl, yl )
  
  horizLines( seq( yl[1], yl[2], 2.5 ), xl, col = 'grey' )
  
  for ( i in 1:length(ex_subj) ) {
    
    sel = dtbf$ID == ex_subj[i]
    xa = dtbf$Time[ sel ]
    ya = dtbf$log_THCCOOH[ sel ]
    
    b("
    lmf = lm( ya ~ xa )
    cf = coef( lmf )
    segments( 0, cf[1],
              max( xa ), cf[1] + cf[2]*max( xa ),
              lwd = 2, col = 'grey' )
    ")
    
    lines( xa, ya, lwd = 2 )
    points( xa, ya, pch = pts[i], bg = clr[i] )
    
  }
  customAxes( xl, yl )
  
  # x-axis
  axis( 1, seq( 0, 32, 4 ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  
  # y-axis
  axis( 2, seq( -7.5, yl[2], 2.5 ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'CN-THCCOOH - log(ng/mg)',
         side = 2, line = 2, cex = 1.25 )
  
  mtext( 'Days Since Last Cannabis Exposure',
         side = 1, line = -2, cex = 1.25, outer = T )
  
  par( xpd = NA )
  legend( -30, 10,
          paste( 'ID =', ex_subj ), 
          pch = pts,
          pt.bg = clr,
          horiz = T,
          bty = 'n',
          cex = 1.25
  )
  par( xpd = F )
  
  dev.off()
  
  # 8.8) Figure 2
  
  png( 'Figure_2.png' )
  
  sel = dtbf$THCCOOH < 1.2
  dtbf$Orig_THCCOOH = dtbf$THCCOOH
  dtbf$Orig_THCCOOH[sel] = 0
  pts = rep( 19, nrow(dtbf) )
  pts[sel] = 1
  
  # Mean over subjects
  plt = dtbf %>% 
    group_by( Time ) %>% 
    summarise(
      R = mean( THCCOOH )
    )
  
  # Obtain model predictions
  sm = summary( bfm$model_fit )
  xa = seq( 0, max( dtbf$Time ) )
  pred = exp( sm$fixed[1,1] - xa*sm$fixed[3,1] )

  ### Raw data
  
  # x11()
  
  x = dtbf$Time
  y = dtbf$THCCOOH
  
  # Create a blank plot
  xl = c( -1, 35 )
  yl = c( -7.5, 750 )
  blankPlot( xl, yl )
  
  horizLines( seq( 150, 750, 150 ), xl, col = 'grey80' )
  
  # Add observations
  points( x, y, pch = 19, col = 'grey' )
  
  # Add mean
  lines( plt$Time, plt$R, lwd = 2, col = 'grey40' )
  points( plt$Time, plt$R, pch = 17 )
  # Add estimates
  lines( xa, pred, lwd = 2 )
  
  # Add axes and labels
  customAxes( xl, yl )
  
  # x-axis
  axis( 1, seq( 0, 32, 4 ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'Days Since Last Cannabis Exposure',
         side = 1, line = 2, cex = 1.25 )
  
  # y-axis
  axis( 2, round( seq( 0, yl[2], 150 ) ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'CN-THCCOOH - ng/mg',
         side = 2, line = 2, cex = 1.25 )
  
  dev.off()
  
  # 8.9) Figure S.1
  
  png( 'Figure_S1.png' )
  
  # x11()
  
  # Generating parameters
  gp = c(
    alpha = 100,
    kappa = .25,
    sigma = 1
  )
  
  # Simulate data
  sim_dat = data.frame(
    Time = seq( 0, 34, length = 100 ),
    epsilon = NA, 
    log_THCCOOH = NA,
    THCCOOH.obs = NA,
    log_THCCOOH.obs = NA
  )
  
  set.seed( 393 )
  sim_dat$epsilon = rnorm( nrow(sim_dat), 0, gp[3] )
  sim_dat$log_THCCOOH = ed_model( gp, sim_dat$Time, log = T )
  sim_dat$log_THCCOOH.obs = sim_dat$log_THCCOOH + sim_dat$epsilon
  sim_dat$THCCOOH.obs = exp( sim_dat$log_THCCOOH.obs )
  sim_dat$THCCOOH.obs.censored = sim_dat$THCCOOH.obs
  
  cut_off = 1
  
  sel = sim_dat$THCCOOH.obs.censored < cut_off
  sim_dat$THCCOOH.obs.censored[sel] = 0
  tr = estimate_ed_lm( sim_dat$Time, sim_dat$THCCOOH.obs.censored )
  
  xl = c( -1, 35 )
  yl = lowerUpper( 1, sim_dat$log_THCCOOH.obs )
  blankPlot( xl, yl )
  
  lines( sim_dat$Time, sim_dat$log_THCCOOH,
         lwd = 2, col = 'blue' )
  
  horizLines( log( cut_off ), xl, lwd = 2, lty = 2 )
  
  clr = rep( 'black', nrow( sim_dat ) )
  clr[ sim_dat$log_THCCOOH.obs < log( cut_off ) ] = 'grey'
  points( sim_dat$Time, sim_dat$log_THCCOOH.obs, 
          pch = 19, col = clr )
  
  sel = sim_dat$log_THCCOOH.obs > log( cut_off )
  lmf = lm( log_THCCOOH.obs ~ Time, data = sim_dat[sel,] )
  est = coef( lmf )
  lines( sim_dat$Time, est[1] + est[2]*sim_dat$Time, 
         col = 'red', lwd = 2 )
  lines( tr$x, log( tr$pred ), col = 'purple', lwd = 2 )
  
  customAxes( xl, yl )
  
  axis( 1, seq( 0, 32, 4 ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'Days Since Last Cannabis Exposure', side = 1, 
         line = 2, cex = 1.25 )
  
  axis( 2, round( seq( yl[1], yl[2], length = 5 ), 1 ), 
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'CN-THCCOOH - ln( ng/mg)', side = 2, 
         line = 2, cex = 1.25 )
  
  legend( 'topright',
          c( 'Observed', 'Censored', 'Generating', 'Estimated', 'Tobit' ),
          fill = c( 'black', 'grey', 'blue', 'red', 'purple' ),
          cex = 1.25, bty = 'n' )
  
  dev.off()
  
  # 8.10) Figure S.2
  
  png( 'Figure_S2.png' )
  
  # x11()
  
  x = dtbf$Time
  y = dtbf$THCCOOH
  
  # Create a blank plot
  xl = c( -1, 35 )
  yl = c( -21, 10 )
  blankPlot( xl, yl )
  
  horizLines( seq( -20, 5, 5 ), xl, col = 'grey80' )
  horizLines( log(1), xl, lwd = 2, lty = 2 )
  
  # Add observations
  clr = rep( 'black', length( y ) )
  clr[ y <= 1 ] = 'grey'
  points( x, log( y ), pch = 19, col = clr )
  
  # 
  tmp = data.frame(
    x = x,
    y = log( y )
  )
  tmp$y[ tmp$y <= 0 ] = NA
  plt_2 = tmp %>% 
    group_by( x ) %>% 
    summarize(
      M = mean( y, na.rm = T )
    )
  
  # Add mean
  lines( plt$Time, log( plt$R ), lwd = 2 )
  lines( plt$Time, plt_2$M, lwd = 2, col = 'red' )
  # Add estimates
  lines( xa, log( pred ), col = 'blue', lwd = 2 )
  
  # Add axes and labels
  customAxes( xl, yl )
  
  # x-axis
  axis( 1, seq( 0, 32, 4 ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'Days Since Last Cannabis Exposure',
         side = 1, line = 2, cex = 1.25 )
  
  # y-axis
  axis( 2, round( seq( -20, 10, 5 ) ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'CN-THCCOOH - ln(ng/mg)',
         side = 2, line = 2, cex = 1.25 )
  
  # Legend
  legend( 'topright', 
          c( 'Mean', 'Mean - censored', 'PK model' ),
          fill = c( 'black', 'red', 'blue' ),
          cex = 1.25, bty = 'n' )
  
  dev.off()
  
}

###
### 9) CUDIT and withdrawal variables
###

if ( all( run_code[c(2,7)] ) ) {
  
  # Extract scores for individual items of CUDIT
  cn = colnames( all_dat )
  CUDIT = all_dat[,paste( 'CUDIT.', 1:8, sep = '' )]
  CUDIT$ID = all_dat$ID
  # Remove missing data
  no_na = apply( CUDIT, 1, function(x) all( !is.na( x ) ) )
  CUDIT = CUDIT[no_na,]
  # Convert to long-form
  # Skip item 1, since all subjects endorsed
  CUDIT = data.frame(
    ID = rep( CUDIT$ID, ncol( CUDIT ) - 2 ),
    Item = rep( paste('Item', 2:8), each = nrow( CUDIT ) ),
    Score = c( CUDIT$CUDIT.2,
               CUDIT$CUDIT.3,
               CUDIT$CUDIT.4,
               CUDIT$CUDIT.5,
               CUDIT$CUDIT.6,
               CUDIT$CUDIT.7,
               CUDIT$CUDIT.8 ),
    Binary_score = 0,
    stringsAsFactors = F
  )
  # Binary scores
  CUDIT$Binary_score[ CUDIT$Score >= 2 ] = 1
  # Rearrange data
  CUDIT = CUDIT %>% arrange( ID, Item )
  
  # Load in RStan
  library( rstan )
  rstan_options(auto_write = TRUE)
  
  # Item and subject indices
  item = createIncrement( CUDIT$Item )
  subject = createIncrement( CUDIT$ID )
  
  # Priors on parameters
  priors = rbind(
    # Log-normal prior on alpha
    c( .5^2, .5 ),
    # Normal prior on beta
    c( 0.0, 1.0 ),
    # Normal prior on theta
    c( 0.0, 1.0 )
  )
  
  # Define a list of inputs for Stan
  stan_dat = list(
    Ni = max( item ),
    Ns = max( subject ),
    No = nrow( CUDIT ),
    item = item,
    subject = subject,
    y = CUDIT$Binary_score,
    priors = priors
  )
  
  warm = 500 # Iterations of warm-up
  niter = 2500 # Number of samples to approximate posterior per chain
  chains = 4 # Number of chains to run in parallel
  
  # Determine run time
  tick()
  
  # Compile a stan script for the 2PL model
  setwd( R_dir )
  sm = stan_model(stanc_ret = stanc_builder(
    "Two_parameter_logistic.stan"))
  
  # Draw samples
  fit = sampling( sm, data = stan_dat, 
                  warmup = warm,
                  iter = warm + niter, 
                  chains = chains,
                  seed = 6002, # For reproducibility
                  control = list(
                    adapt_delta = .95,
                    max_treedepth = 15
                  )
  )
  tock()
  print( run_time )
  
  # Extract posterior samples
  post = as.matrix( fit )
  
  # Extract subject estimates
  cn = colnames( post )
  sel = grep( 'theta', cn )
  
  CUDIT$Subject = subject
  SS = CUDIT %>% 
    group_by( ID ) %>% 
    summarize( 
      SS = sum( Binary_score ),
      Subject = unique( Subject )
    )
  SS$Theta = 
    colMeans( post[,sel] )[ SS$Subject ]
  
  all_est$CUDIT = NA
  for ( s in 1:nrow( all_est ) ) {
    sel = all_dat$ID == all_est$ID[s];
    all_est$CUDIT[s] = unique( all_dat$CUDIT.SS[sel] )
  }
  all_est$Theta = NA
  for ( i in 1:nrow( all_est ) ) {
    sel = SS$ID == all_est$ID[i]
  all_est$Theta[i] = SS$Theta[sel]  
  }
  
  qp = function( x, y, labels ) {
    
    zx = my_standardize( x )
    zy = my_standardize( y )
    
    pl = lowerUpper( 1, c( zx, zy ) )
    blankPlot( pl, pl )
    
    xl = range( x )
    xv = list( x = round( seq( xl[1], xl[2], length = 5 ), 1 ) )
    xv$zx = ( xv$x - attributes( zx )$mean )/attributes( zx )$scale
    yl = range( y )
    yv = list( y = round( seq( yl[1], yl[2], length = 5 ), 1 ) )
    yv$zy = ( yv$y - attributes( zy )$mean )/attributes( zy )$scale
    
    lmf = lm( zy ~ -1 + zx )
    cf = coef( lmf )
    
    segments( xv$zx[1], xv$zx[1] * cf,
              xv$zx[5], xv$zx[5] * cf,
              lwd = 2, col = 'grey' )
    
    points( zx, zy, pch = 19 )
    
    legend( 'topright',
            paste( 'R =', round( cf, 2 ) ),
            bty = 'n',
            cex = 1.25 )
    
    customAxes( pl, pl )
    
    axis( 1, round( xv$zx, 1 ), 
          tick = F, line = -1.5, cex.axis = 1.25 )
    mtext( labels[1], side = 1, line = 2, cex = 1.25 )
    
    axis( 2, round( yv$zy, 1 ), 
          tick = F, line = -1.5, cex.axis = 1.25 )
    mtext( labels[2], side = 2, line = 2, cex = 1.25 )
    
  }
  
  x11( width = 12, height = 12 )
  layout( matrix( 1:4, 2, 2, byrow = F ) )
    
  qp( all_est$Theta, all_est$Start_point, 
      c( 'Cannabis disorder propensity', 'Starting level of CN-THCCOOH' )  )
  qp( all_est$Theta, all_est$Elimination_rate, 
      c( 'Cannabis disorder propensity', 'Elimination rate' )  )
  qp( all_est$CUDIT, all_est$Start_point, 
      c( 'CUDIT (Summed scores)', 'Starting level of CN-THCCOOH' )  )
  qp( all_est$CUDIT, all_est$Elimination_rate, 
      c( 'CUDIT (Summed scores)', 'Elimination rate' )  )
  
  dtbp = dtbf %>% 
    group_by( Time ) %>% 
    summarize( 
      WI = mean( Withdrawal_intensity, na.rm = T ), 
      WNI = mean( Withdrawal_negative_impact, na.rm = T )
    )
  
  x11( width = 12 );
  layout( cbind( 1, 2 ) )
  
  xl = c( -1, 35 )
  yl = lowerUpper( 10, dtbp$WI )
  yl[1] = -2.5
  blankPlot( xl, yl )
  
  points( dtbf$Time, dtbf$Withdrawal_intensity, 
          pch = 19, col = 'grey' )
  
  lines( dtbp$Time, dtbp$WI, lwd = 2 )
  
  customAxes( xl, yl )
  axis( 1, seq( 0, 34, 6 ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'Days Since Last Cannabis Exposure',
         side = 1, line = 2, cex = 1.25 )
  
  # y-axis
  axis( 2, round( seq( 0, yl[2], 10 ) ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'Intensity of withdrawal symptoms',
         side = 2, line = 2, cex = 1.25 )
  
  
  xl = c( -1, 35 )
  yl = lowerUpper( 10, dtbp$WNI )
  yl[1] = -2.5
  blankPlot( xl, yl )
  
  points( dtbf$Time, dtbf$Withdrawal_negative_impact, 
          pch = 19, col = 'grey' )
  
  lines( dtbp$Time, dtbp$WNI, lwd = 2 )
  
  customAxes( xl, yl )
  axis( 1, seq( 0, 34, 6 ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'Days Since Last Cannabis Exposure',
         side = 1, line = 2, cex = 1.25 )
  
  # y-axis
  axis( 2, round( seq( 0, yl[2], 10 ) ),
        tick = F, line = -1.5, cex.axis = 1.25 )
  mtext( 'Negative impact of withdrawal symptoms',
         side = 2, line = 2, cex = 1.25 )
  
  dtba = dtbf
  dtba$EM.Start_point = NA
  dtba$EM.Elimination_rate = NA
  for ( s in 1:nrow( all_est ) ) {
    sel = dtba$ID == all_est$ID[s]
    dtba$EM.Start_point[sel] = all_est$Log_start_point[s]
    dtba$EM.Elimination_rate[sel] = all_est$Elimination_rate[s]
  }
  res1 = lme4::lmer(
    Withdrawal_intensity ~ 1 + Time + EM.Start_point + EM.Elimination_rate + 
      (1|ID), data = dtba )
  res2 = lme4::lmer(
    Withdrawal_negative_impact ~ 1 + Time + EM.Start_point + EM.Elimination_rate + 
      (1|ID), data = dtba )
  
}

setwd( R_dir )
